{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkAwQ4Z_AZQt"
   },
   "source": [
    "### Install sentence transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11067,
     "status": "ok",
     "timestamp": 1612435487909,
     "user": {
      "displayName": "ATHANASIOS BRIAKOS",
      "photoUrl": "",
      "userId": "17932687169031889732"
     },
     "user_tz": -120
    },
    "id": "ZoUnSU2MAfS4",
    "outputId": "c77f3528-0682-40cd-875c-7e697150ca52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/e2/84d6acfcee2d83164149778a33b6bdd1a74e1bcb59b2b2cd1b861359b339/sentence-transformers-0.4.1.2.tar.gz (64kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 5.6MB/s \n",
      "\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8MB 14.8MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.7.0+cu101)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.19.5)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 45.3MB/s \n",
      "\u001b[?25hCollecting tokenizers==0.9.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 41.6MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.8)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 39.1MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.8)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
      "Building wheels for collected packages: sentence-transformers, sacremoses\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-0.4.1.2-cp36-none-any.whl size=103068 sha256=40f75673cefaa02e0579a3cc942926e54b0a276af513d34a40e7c3a869583d39\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/33/d1/5703dd56199c09d4a1b41e0c07fb4e7765a84d787cbdc48ac3\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=3e82668039628444a59593751ec1b89ab1f669b6bf0a1f234cef9d0ce1e46029\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sentence-transformers sacremoses\n",
      "Installing collected packages: tokenizers, sacremoses, transformers, sentencepiece, sentence-transformers\n",
      "Successfully installed sacremoses-0.0.43 sentence-transformers-0.4.1.2 sentencepiece-0.1.95 tokenizers-0.9.4 transformers-4.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-UkyvR9egcs"
   },
   "source": [
    "### Useful imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18167,
     "status": "ok",
     "timestamp": 1612435495018,
     "user": {
      "displayName": "ATHANASIOS BRIAKOS",
      "photoUrl": "",
      "userId": "17932687169031889732"
     },
     "user_tz": -120
    },
    "id": "Yhr83nwuejYo",
    "outputId": "656e792b-9b24-4fdc-cddf-023a345d7776"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json,glob,nltk,copy,torch,time\n",
    "from scipy import spatial\n",
    "from queue import PriorityQueue\n",
    "from sentence_transformers import SentenceTransformer,util\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ct20dwbsmhb"
   },
   "source": [
    "### Retrieve dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58216,
     "status": "ok",
     "timestamp": 1612435535074,
     "user": {
      "displayName": "ATHANASIOS BRIAKOS",
      "photoUrl": "",
      "userId": "17932687169031889732"
     },
     "user_tz": -120
    },
    "id": "1Rmi5RJRibuG",
    "outputId": "cc8f68dc-028e-4b0b-cd77-49c161e21420"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-04 10:44:53--  https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2020-03-13.tar.gz\n",
      "Resolving ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com (ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com)... 52.218.153.153\n",
      "Connecting to ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com (ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com)|52.218.153.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 278921140 (266M) [application/x-tar]\n",
      "Saving to: ‘cord-19_2020-03-13.tar.gz’\n",
      "\n",
      "cord-19_2020-03-13. 100%[===================>] 266.00M  55.3MB/s    in 5.1s    \n",
      "\n",
      "2021-02-04 10:44:59 (52.3 MB/s) - ‘cord-19_2020-03-13.tar.gz’ saved [278921140/278921140]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases/cord-19_2020-03-13.tar.gz\n",
    "!tar -xf cord-19_2020-03-13.tar.gz\n",
    "!tar -xf 2020-03-13/comm_use_subset.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsWRMwlR6DwQ"
   },
   "source": [
    "### Prepare GPU Cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58217,
     "status": "ok",
     "timestamp": 1612435535080,
     "user": {
      "displayName": "ATHANASIOS BRIAKOS",
      "photoUrl": "",
      "userId": "17932687169031889732"
     },
     "user_tz": -120
    },
    "id": "hN25lAzQ6Jws",
    "outputId": "7c2c8bbc-6bd4-442a-86c0-782ea687ab15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for running: \n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device available for running: \")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i798G_ziUFqQ"
   },
   "source": [
    "### Read JSON files and store title,abstract and text of each article into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8kfpcKOZ-a6S"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "files = glob.glob('comm_use_subset/*', recursive=True)\n",
    "number_of_articles = len(files)\n",
    "bound = 9000\n",
    "\n",
    "for single_file in files[0:bound]:\n",
    "  with open(single_file, 'r') as f:\n",
    "    json_file = json.load(f)\n",
    "    \n",
    "    # Retrieve title\n",
    "    title = json_file[\"metadata\"][\"title\"]\n",
    "\n",
    "    # Retrieve abstracts\n",
    "    abstracts = []\n",
    "    if len(json_file[\"abstract\"]) != 0 :\n",
    "      for abstract in json_file[\"abstract\"]:\n",
    "        abstracts.append(abstract[\"text\"])\n",
    "\n",
    "    # Retrieve texts\n",
    "    texts = []\n",
    "    for text in json_file[\"body_text\"]:\n",
    "      texts.append(text[\"text\"])\n",
    "\n",
    "    data.append([title,abstracts,texts])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYl7u849jyJ2"
   },
   "source": [
    "### Convert corpus to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IY127-hMhn98"
   },
   "outputs": [],
   "source": [
    "# For each article\n",
    "for article in range(bound):\n",
    "\n",
    "  # Title section\n",
    "  data[article][0] = nltk.sent_tokenize(data[article][0])\n",
    "\n",
    "  # Abstracts section \n",
    "  for abstract in range(len(data[article][1])): \n",
    "    data[article][1][abstract] = nltk.sent_tokenize(data[article][1][abstract])\n",
    "   \n",
    "  # Texts section\n",
    "  for text in range(len(data[article][2])):\n",
    "    data[article][2][text] = nltk.sent_tokenize(data[article][2][text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fVD3jkJU2AJ"
   },
   "source": [
    "### Transform only each article's title and abstract's sentences to embeddings with second sentence trasformer 'stsb-roberta-base' model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 661307,
     "status": "ok",
     "timestamp": 1612436138180,
     "user": {
      "displayName": "ATHANASIOS BRIAKOS",
      "photoUrl": "",
      "userId": "17932687169031889732"
     },
     "user_tz": -120
    },
    "id": "I2xV5cD_U8pO",
    "outputId": "b9f126cf-8888-4625-849e-a72f6d2784b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 461M/461M [00:25<00:00, 18.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 8.6 minutes\n"
     ]
    }
   ],
   "source": [
    "second_start_time = time.time()\n",
    "\n",
    "# Declare our second sentence transformer model and pass it to appropriate device\n",
    "second_model = SentenceTransformer('stsb-roberta-base').to(device)\n",
    "\n",
    "# Here we'll put for each article sentences of abstract and body text\n",
    "abstract_embeddings = [[] for i in range(bound)]\n",
    "all_abstracts = [[] for i in range(bound)]\n",
    "title_embeddings = []\n",
    "\n",
    "# For each article\n",
    "for article in range(bound):\n",
    "  \n",
    "  # Get title and abstract and of each article\n",
    "  title_ = data[article][0]\n",
    "  abstract_ = data[article][1]\n",
    "\n",
    "  # Process to keep abstract and sentences of each article in a big list\n",
    "  for abstract in abstract_:\n",
    "    for sentence in abstract:\n",
    "      all_abstracts[article].append(sentence)\n",
    "  \n",
    "  # Transform title to embeddings \n",
    "  if len(title_) != 0:\n",
    "    title_embeddings.append(second_model.encode(title_,convert_to_tensor=True))\n",
    "\n",
    "  if len(abstract_) != 0:\n",
    "    # Transform abstract's sentences to embeddings \n",
    "    abstract_embeddings[article].append(second_model.encode(all_abstracts[article],convert_to_tensor=True))\n",
    "\n",
    "    # Convert all sentence embeddings to a 2D pytorch tensor\n",
    "    abstract_embeddings[article] = torch.cat(abstract_embeddings[article]) \n",
    "  \n",
    "# Check elapsed time of second model\n",
    "second_model_time = (time.time() - second_start_time)/60\n",
    "print(\"Elapsed time: %s minutes\" % (round(second_model_time,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8N9sewtpXLxz"
   },
   "source": [
    "### Declare our queries and tranform them to embeddings based on our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 661306,
     "status": "ok",
     "timestamp": 1612436138184,
     "user": {
      "displayName": "ATHANASIOS BRIAKOS",
      "photoUrl": "",
      "userId": "17932687169031889732"
     },
     "user_tz": -120
    },
    "id": "rWl9zu5ix4hO",
    "outputId": "ad196d95-fceb-406a-aeca-150ce6c45a3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 2nd model... Number Of Queries: 8  Query Embedding's Length: 768\n"
     ]
    }
   ],
   "source": [
    "queries = ['What are the coronoviruses?','What was discovered in Wuhuan in December 2019?',\n",
    "           'What is Coronovirus Disease 2019?','What is COVID-19?','What is caused by SARS-COV2?',\n",
    "           'How is COVID-19 spread?','Where was COVID-19 discovered?','How does coronavirus spread?']\n",
    "\n",
    "second_queries_embeddings = second_model.encode(queries,convert_to_tensor=True)\n",
    "\n",
    "print(\"For 2nd model... Number Of Queries:\",len(second_queries_embeddings),\" Query Embedding's Length:\",len(second_queries_embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HlsshWlKi6M"
   },
   "source": [
    "### Now we are going implement function, which search body texts from articles which are derived from those which their abstracts or/and titles gave us the best cosine similarities, 10% percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcgg0jsILChT"
   },
   "outputs": [],
   "source": [
    "def cos_sim(v1,v2):\n",
    "  \"\"\"\n",
    "  Function which calculates cosine similarity between two vectors.\n",
    "  \"\"\"\n",
    "  res = 1 - spatial.distance.cosine(v1,v2)\n",
    "  return float(round(res, 2))\n",
    "\n",
    "def reduce_searching(query_embedding,titles,abstracts,query_sentence,data_,ratio,title_or_abstract):\n",
    "  \"\"\"\n",
    "  Function which gives the closest vector (metric:cosine similarity) as an answer, \n",
    "  but isn't searching all articles (brutely forced). We search only the top ratio% \n",
    "  of articles which were closer based on average cosine similarity of title section or abstract\n",
    "  section or both of them.\n",
    "  \n",
    "  Variable title_or_abstract:\n",
    "  Consider only title if -1\n",
    "  Consider only abstract if 1\n",
    "  Consider both of them if 0\n",
    "  \"\"\"\n",
    "\n",
    "  # Declare our priority queues\n",
    "  pq = PriorityQueue() \n",
    "  answer_pq = PriorityQueue()\n",
    "  \n",
    "  # For each article\n",
    "  for index,article in enumerate(zip(titles,abstracts)):\n",
    "\n",
    "    title_cos_sim = 0\n",
    "    abstract_cos_sim = 0\n",
    "    title = article[0]\n",
    "    abstract = article[1]\n",
    "\n",
    "    # For each sentence of title section \n",
    "    for sentence in title:\n",
    "      title_cos_sim += cos_sim(query_embedding,sentence)\n",
    "    # If title isn't empty calculate average abstract cosine similarity\n",
    "    if len(title) != 0: \n",
    "      title_cos_sim = round(title_cos_sim/len(title),2)   \n",
    "\n",
    "    # For each text of abstract section \n",
    "    for sentence in abstract:\n",
    "      abstract_cos_sim += cos_sim(query_embedding,sentence)\n",
    "    # If abstract isn't empty calculate average abstract cosine similarity\n",
    "    if len(abstract) != 0:\n",
    "      abstract_cos_sim = round(abstract_cos_sim/len(abstract),2)   \n",
    "\n",
    "    # Calculate average cosine similarity (of title and abstract) \n",
    "    total_cos_sim = round(((title_cos_sim + abstract_cos_sim)/2),2)\n",
    "        \n",
    "    # Insert into priority queue pair of cosine similarity of abstract and article's index\n",
    "    if title_or_abstract == 1:\n",
    "      pq.put((abstract_cos_sim*(-1),index))\n",
    "    # Insert into priority queue pair of cosine similarity of title and article's index\n",
    "    elif title_or_abstract == -1:\n",
    "      pq.put((title_cos_sim*(-1),index))\n",
    "    # Insert into priority queue pair of average cosine similarity (of title and abstract) and article's index\n",
    "    else:\n",
    "      pq.put((total_cos_sim*(-1),index))\n",
    "\n",
    "  # Number of articles that we gonna search further (their body text)\n",
    "  search_articles = round(ratio*len(data_))\n",
    "\n",
    "  # Declare empty lists in order to save inside them embeddings and sentences of abstracts\n",
    "  body_text_embeddings = [[] for i in range(search_articles)]\n",
    "  body_texts = [[] for i in range(search_articles)]\n",
    "\n",
    "  # Search thorougly articles which had big similarity on title and abstract section  \n",
    "  for i in range(search_articles):\n",
    "    _,index = pq.get()\n",
    "    body_text = data_[index][2]\n",
    "\n",
    "    # Stack all body_text's sentences to a temporary list\n",
    "    for text in body_text:\n",
    "      for sentence in text:\n",
    "        body_texts[i].append(sentence)\n",
    "    \n",
    "    # Transform sentences to embeddings \n",
    "    body_text_embeddings[i].append(second_model.encode(body_texts[i],convert_to_tensor=True))\n",
    "\n",
    "    # Convert all sentence embeddings to a 2D pytorch tensor\n",
    "    body_text_embeddings[i] = torch.cat(body_text_embeddings[i])\n",
    "\n",
    "    # For each sentence embedding of body_text section \n",
    "    for sen_index,sentence in enumerate(body_text_embeddings[i]): \n",
    "      answer_pq.put((cos_sim(query_embedding,sentence)*(-1),(index,i,sen_index)))\n",
    "\n",
    "  # Display results \n",
    "  print(\"Query:\", query_sentence,\"\\n\")\n",
    "\n",
    "  for i in range(4):  \n",
    "    _,res = answer_pq.get()\n",
    "\n",
    "    print(\"Answer\",i+1,\":\",body_texts[res[1]][res[2]])\n",
    "    print(\"From article with title:\",end=\" \")\n",
    "    for text in data_[res[0]][0]:\n",
    "      print(text,end =\" \")\n",
    "\n",
    "    print(\"\\nCosine Similarity:\", -_,\"\\n\")\n",
    "  print(\"--------------------------------------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxYc3nXzYzgm"
   },
   "source": [
    "### Test our second model with our non Brute Force function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3707950,
     "status": "ok",
     "timestamp": 1612439184833,
     "user": {
      "displayName": "ATHANASIOS BRIAKOS",
      "photoUrl": "",
      "userId": "17932687169031889732"
     },
     "user_tz": -120
    },
    "id": "7bsPHNA9UIqa",
    "outputId": "4f58a277-557c-41e0-ae30-8a26f61ec468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the coronoviruses? \n",
      "\n",
      "Answer 1 : With some coronaviruses, e.g.\n",
      "From article with title: Differential Sensitivity of Bat Cells to Infection by Enveloped RNA Viruses: Coronaviruses, Paramyxoviruses, Filoviruses, and Influenza Viruses \n",
      "Cosine Similarity: 0.73 \n",
      "\n",
      "Answer 2 : Coronaviruses.\n",
      "From article with title: A viral metagenomic survey identifies known and novel mammalian viruses in bats from Saudi Arabia \n",
      "Cosine Similarity: 0.68 \n",
      "\n",
      "Answer 3 : The Coronaviridae Family.\n",
      "From article with title: A Genome-Wide Analysis of RNA Pseudoknots That Stimulate Efficient −1 Ribosomal Frameshifting or Readthrough in Animal Viruses \n",
      "Cosine Similarity: 0.68 \n",
      "\n",
      "Answer 4 : coronaviruses.\n",
      "From article with title: Action Mechanisms of Lithium Chloride on Cell Infection by Transmissible Gastroenteritis Coronavirus \n",
      "Cosine Similarity: 0.67 \n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Query: What was discovered in Wuhuan in December 2019? \n",
      "\n",
      "Answer 1 : In December 2019, a cluster of pneumonia of unknown etiology was detected in Wuhan City, Hubei Province of China.\n",
      "From article with title: Clinical Medicine Characteristics of and Public Health Responses to the Coronavirus Disease 2019 Outbreak in China \n",
      "Cosine Similarity: 0.62 \n",
      "\n",
      "Answer 2 : , 2019 .\n",
      "From article with title: A Novel Gene Underlies Bleomycin-Response Variation in Caenorhabditis elegans \n",
      "Cosine Similarity: 0.56 \n",
      "\n",
      "Answer 3 : , 2019 .\n",
      "From article with title: A Novel Gene Underlies Bleomycin-Response Variation in Caenorhabditis elegans \n",
      "Cosine Similarity: 0.56 \n",
      "\n",
      "Answer 4 : Methods This prospective observational study was performed from September to December 2015.\n",
      "From article with title: ESICM LIVES 2016: part one Oral Sessions. ARDS: CLINICAL STUDIES A1 Identification of distinct endophenotypes in patients with acute respiratory distress syndrome by unbiased cluster analysis, and their association with mortality \n",
      "Cosine Similarity: 0.55 \n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Query: What is Coronovirus Disease 2019? \n",
      "\n",
      "Answer 1 : Thereafter, this disease was named Coronavirus Disease 2019 (COVID-19) by World Health Organization (WHO), and the causative virus was designated as SARS-CoV-2 by the International Committee on Taxonomy of Viruses [6] .\n",
      "From article with title: Clinical Medicine Characteristics of and Public Health Responses to the Coronavirus Disease 2019 Outbreak in China \n",
      "Cosine Similarity: 0.62 \n",
      "\n",
      "Answer 2 : Early January 2020, a novel coronavirus (2019-nCoV) was identified as the infectious agent causing an outbreak of viral pneumonia in Wuhan, China, where the first cases had their symptom onset in December 2019 [1] .\n",
      "From article with title: Correspondence: Jantien A Backer (jantien.backer@rivm.nl) \n",
      "Cosine Similarity: 0.62 \n",
      "\n",
      "Answer 3 : Coronavirus.\n",
      "From article with title: Population genetics, community of parasites, and resistance to rodenticides in an urban brown rat (Rattus norvegicus) population \n",
      "Cosine Similarity: 0.62 \n",
      "\n",
      "Answer 4 : On 7 January 2020 investigators in China identified the etiological agent of the epidemic as a previously unknown coronavirus, and it was given the designation 2019-nCoV (for 2019 novel coronavirus) [8] .\n",
      "From article with title: viruses Perspective Potential Maternal and Infant Outcomes from Coronavirus 2019-nCoV (SARS-CoV-2) Infecting Pregnant Women: Lessons from SARS, MERS, and Other Human Coronavirus Infections \n",
      "Cosine Similarity: 0.61 \n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Query: What is COVID-19? \n",
      "\n",
      "Answer 1 : 19\n",
      "From article with title: Culturing of respiratory viruses in well-differentiated pseudostratified human airway epithelium as a tool to detect unknown viruses \n",
      "Cosine Similarity: 0.74 \n",
      "\n",
      "Answer 2 : 19 ).\n",
      "From article with title: ESICM LIVES 2016: part one Oral Sessions. ARDS: CLINICAL STUDIES A1 Identification of distinct endophenotypes in patients with acute respiratory distress syndrome by unbiased cluster analysis, and their association with mortality \n",
      "Cosine Similarity: 0.73 \n",
      "\n",
      "Answer 3 : (S19)\n",
      "From article with title: Disease dynamics in a stochastic network game: a little empathy goes a long way in averting outbreaks Supplementary Material \n",
      "Cosine Similarity: 0.71 \n",
      "\n",
      "Answer 4 : [19] .\n",
      "From article with title: Antioxidant Status and Immune Activity of Glycyrrhizin in Allergic Rhinitis Mice \n",
      "Cosine Similarity: 0.71 \n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Query: What is caused by SARS-COV2? \n",
      "\n",
      "Answer 1 : CCL2 is believed to be responsible for respiratory inflammatory symptoms in SARS patients [181] .\n",
      "From article with title: Human Coronaviruses: A Review of Virus-Host Interactions \n",
      "Cosine Similarity: 0.64 \n",
      "\n",
      "Answer 2 : Two isoforms of COX were described: COX-1 and COX-2.\n",
      "From article with title: Activation of Interleukin-32 Pro-Inflammatory Pathway in Response to Influenza A Virus Infection \n",
      "Cosine Similarity: 0.62 \n",
      "\n",
      "Answer 3 : The recent emergence of a SARS-like CoV called Middle East Respiratory Syndrome coronavirus has underscored the need to understand the mechanisms behind CoV pathogenicity 2 .\n",
      "From article with title: SARS-Coronavirus Open Reading Frame-3a drives multimodal necrotic cell death \n",
      "Cosine Similarity: 0.6 \n",
      "\n",
      "Answer 4 : Severe acute respiratory syndrome (SARS) is caused by a group 2 CoV termed SARS-CoV that emerged from China in late 2002 [6] [7] [8] [9] [10] [11] [12] [13] .\n",
      "From article with title: Depletion of Alveolar Macrophages Ameliorates Virus- Induced Disease following a Pulmonary Coronavirus Infection \n",
      "Cosine Similarity: 0.59 \n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Query: How is COVID-19 spread? \n",
      "\n",
      "Answer 1 : 19\n",
      "From article with title: Culturing of respiratory viruses in well-differentiated pseudostratified human airway epithelium as a tool to detect unknown viruses \n",
      "Cosine Similarity: 0.66 \n",
      "\n",
      "Answer 2 : 19 ).\n",
      "From article with title: NAR Breakthrough Article A novel role for poly(C) binding proteins in programmed ribosomal frameshifting \n",
      "Cosine Similarity: 0.63 \n",
      "\n",
      "Answer 3 : 19 ).\n",
      "From article with title: ESICM LIVES 2016: part one Oral Sessions. ARDS: CLINICAL STUDIES A1 Identification of distinct endophenotypes in patients with acute respiratory distress syndrome by unbiased cluster analysis, and their association with mortality \n",
      "Cosine Similarity: 0.63 \n",
      "\n",
      "Answer 4 : The COVID-19 has then rapidly spread to all over China and the world.\n",
      "From article with title: The novel coronavirus outbreak in Wuhan, China \n",
      "Cosine Similarity: 0.62 \n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Query: Where was COVID-19 discovered? \n",
      "\n",
      "Answer 1 : The 2019-nCoV receptor(s) is currently unknown.\n",
      "From article with title: Outbreak 2019-nCoV (Wuhan virus), a novel Coronavirus: human-to-human transmission, travel-related cases, and vaccine readiness \n",
      "Cosine Similarity: 0.6 \n",
      "\n",
      "Answer 2 : 19% (1,714,925/1,922,699)\n",
      "From article with title: Analysis of Epidemiological Characteristics of Notifiable Diseases Reported in Children Aged 0-14 Years from 2008 to 2017 in Zhejiang Province, China \n",
      "Cosine Similarity: 0.59 \n",
      "\n",
      "Answer 3 : LE19P directs the synthesis of a de novo initiated 19-nt product.\n",
      "From article with title: Subgenomic promoter recognition by the norovirus RNA-dependent RNA polymerases \n",
      "Cosine Similarity: 0.59 \n",
      "\n",
      "Answer 4 : New data for the Arg230Cys polymorphism were generated for 19 Amerindian populations (n = 229) from Meso/Central America and South America (Table 1) .\n",
      "From article with title: Evolutionary Responses to a Constructed Niche: Ancient Mesoamericans as a Model of Gene-Culture Coevolution \n",
      "Cosine Similarity: 0.58 \n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Query: How does coronavirus spread? \n",
      "\n",
      "Answer 1 : Results based on the coronavirus dataset.\n",
      "From article with title: Comparative studies of alignment, alignment-free and SVM based approaches for predicting the hosts of viruses based on viral sequences OPEN \n",
      "Cosine Similarity: 0.67 \n",
      "\n",
      "Answer 2 : Parainfluenza (a), coronavirus (b), and metapneumovirus (c) infection.\n",
      "From article with title: Respiratory viral infections and the risk of rheumatoid arthritis \n",
      "Cosine Similarity: 0.63 \n",
      "\n",
      "Answer 3 : coronavirus.\n",
      "From article with title: Tropical Medicine and Infectious Disease Potential Intermediate Hosts for Coronavirus Transmission: No Evidence of Clade 2c Coronaviruses in Domestic Livestock from Ghana \n",
      "Cosine Similarity: 0.63 \n",
      "\n",
      "Answer 4 : Recently, a novel coronavirus has been identified in patients with severe acute respiratory illness [1, 2] .\n",
      "From article with title: A novel coronavirus capable of lethal human infections: an emerging picture \n",
      "Cosine Similarity: 0.63 \n",
      "\n",
      "--------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Elapsed time: 50.8 minutes\n"
     ]
    }
   ],
   "source": [
    "second_start_time = time.time()\n",
    "\n",
    "for i in range(len(queries)):\n",
    "  reduce_searching(second_queries_embeddings[i],title_embeddings,abstract_embeddings,queries[i],data,0.1,0)\n",
    "\n",
    "second_model_time = (time.time() - second_start_time)/60\n",
    "print(\"Elapsed time: %s minutes\" % (round(second_model_time,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiSIylj27ZVy"
   },
   "source": [
    "### Notes & Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4pQJW8Hx9Ve"
   },
   "source": [
    ">Note that our dataset is the initial-first release of CORD-19 dataset, 2020-03-13, which is the smallest possible dataset with 9000 articles. \n",
    "You can find it here: [CORD-19_Releases](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXa5hyf17bUY"
   },
   "source": [
    "#### Just a recap of what we presented in this notebook!\n",
    "\n",
    ">In contrast with first notebook,where we use a brute force way, in order to search all sentences of each article's abstract \n",
    "and body text section and compare them via cosine similarity metric with a single query embedding and thus return the most \n",
    "'similar' answer, here in this notebook we tried to reduce searching so as to save time. \n",
    "\n",
    ">Let's explain, of course our whole concept...\n",
    ">1. Initially, after corpus's tokenization to sentences, we obtained all article's titles and abstracts and transform them to \n",
    "sentence embeddings. \n",
    ">2. So we have on our hands, only a limited information about each article, cause we have ignored core text of each article. So, by \n",
    "reduce_searching's function implementation we have the privilege to search only a specific percentage of all articles's body texts. \n",
    "We limit our searching by simply compare the appropriate query embedding only with sentences which derived from tile and abstract \n",
    "section of each article.\n",
    "In that point, let us just note that we have three options: \n",
    "\n",
    ">> A) Consider only title section.\n",
    "\n",
    ">> B) Consider only abstract section.\n",
    "\n",
    ">> C) Consider both of abstract and title section.\n",
    "\n",
    ">>We experimented with each option, but we decided to keep as much as possible information we could. Also, note that in the \n",
    "scenario that we consider only title section we saved a lot of time, but model returns titles from papers that didn't match \n",
    "question's context. In contrast, with considering abstract section, where it exists a small summary of paper's context, we have \n",
    "more probabilities to find close-similar words, cause there are more than one sentences at most cases.\n",
    "\n",
    ">3. We have to pin that we have the ability to determine the percentage of articles which have best cosine similarity between \n",
    "query's embedding and each sentence from title's or/and abstract's section. We decide to set this ratio to 10%.\n",
    ">4. With the previous explanations, we could easily say that we have a crucial difference off between time and wasted resources between our two \n",
    "notebooks.\n",
    "\n",
    ">>a) Where in first we follow a more brute force way, in order to search every possible information from articles, but that \n",
    "cost us to time, approximately whole process endured 2 hours, with total use of Cuda GPU. In addition, just to remind in \n",
    "first notebook we used pretrained sbert model,'stsb-bert-base', in order to transform sentences to embeddings of size 768. \n",
    "\n",
    ">>b) On the other hand in current, second, notebook we tried to save time, but this cost us in accuracy of answers to our questions \n",
    "about COVID-19. So to be more specific, in this notebook we managed to save half time, 1 hour, thus our whole process here lasts \n",
    "approximately 1 hour. Last but not least, in current notebook we used 'stsb-roberta-base' model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyG84B7azROA"
   },
   "source": [
    ">Here you can check all the available SBERT pretrained models that somebody can use [Sentence Transformers PreTrained Models](https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/edit#gid=0), order to transform a sentence to embedding. \n",
    "\n",
    ">In the above link you can observe by yourself that 'stdb-bert-base' and 'stsb-roberta-base', if you compare them that they don't have so many differences. Let's be more detailed...both of these models have same same speed, 2300 sentences per second on V100 GPU and about their performances 'stsb-roberta-base' is slightly better with score 85,44 against 85,14 of 'stsb-bert-base'. Let's emphasize that these specific models are among the best, based on their performance models, which sentence transformers library provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ijg02cB00bwn"
   },
   "source": [
    ">So we can conclude that despite the fact that our two sentence pretrained models don't differ too much, we followed in each one a different approach and thus resulting to have some major differences between our two notebooks. So let's analyze some crucial criteria, with which we gonna compare our two approaches:\n",
    "\n",
    "1. Time \n",
    ">First criteria, that we pick, in order to compare our two approaches is time that each notebook endured. So, for our first notebook, where we followed a more brute force way, our whole notebook lasts approximately 2 hours. In contrast to first notebook, on our second notebook we tried to save time and we achieved that, cause elapsed time was 1 hour. That difference between time was more than expected. Practically, our mindset behind second notebook was to search a small part (10%) of whole articles, like a heuristic function, which build with help of title and abstract section.\n",
    "\n",
    "2. Computing Power\n",
    ">Second criteria, that we pick, in order to compare our two approaches is computing power. With term 'Computing Power' we mean two critical subfactors,percentage of usage of hardware accelerator (GPU Cuda) of Google Colab and memory limit usage of Google Colab. \n",
    "\n",
    ">> In first notebook, we have to note that during transforming sentences to embeddings we waste all possible computational resources, specifically we had total usage of GPU,marginally use all available Google Colab RAM,~12GB, and \n",
    "obviously to save,these embeddings,we had to sacrifice approximately ~6.5GB in our Google Drive. -Note that Google Drive provides for free 15GB memory storage-.\n",
    "\n",
    ">> On the other hand, in second notebook due to the fact we transform initially only title and abstract section of all articles and then only 10% of body texts of all articles, we managed to save a lot of memory, approximately we utilized only ~1GB for storage. Furthermore, by watching closely GPU we conclude that in this notebook Google didn't provide to us total usage of CUDA!\n",
    "    \n",
    ">> So to sum up, as a result we can refer that we save a lot of computing power in second notebook rather than in first.\n",
    "\n",
    "3. Accuracy\n",
    ">Third possible criteria, that we could pick, in order to compare our two approaches is accuracy and efficiency of our results. Generally, our two notebooks with our non-official evaluation were pretty close as performance, with some slight differences. We believed marginally this notebook performed more efficiently!Although, both of notebooks had the same problem (with cosine \n",
    "similarity, which we discussed in previous notebook), they have also some exceptions, in which we obtained brilliant answers! We analyzed these cases for previous notebook so let us discuss for current notebook:\n",
    "\n",
    "---\n",
    "> Query: \"What was discovered in Wuhuan in December 2019?\"\n",
    "\n",
    ">  Answer: \"In December 2019, a cluster of pneumonia of unknown etiology was detected in Wuhan City, Hubei Province of China.\"\n",
    "\n",
    ">  With cosine similarity score 0.62.  \n",
    "---\n",
    "> Query: \"What is Coronovirus Disease 2019?\"\n",
    "\n",
    "> Answer: \"Thereafter, this disease was named Coronavirus Disease 2019 (COVID-19) by World Health Organization (WHO), and the causative virus was designated as SARS-CoV-2 by the International Committee on Taxonomy of Viruses.\"\n",
    "\n",
    "> With cosine similarity score 0.62. \n",
    "---\n",
    ">Those two mentioned answers were really relevant and satisfying, although cosine similarity hasn't pretty high value!\n",
    "---\n",
    ">Let us just explain, as last note, that for both notebooks about their results, we may not retrieve in all questions, desirable \n",
    "answers, but at most of our examples the retrieved title of paper, had as main concept COVID-19, so if all this search was for \n",
    "information extraction or retrieval, we achieved in high rate to obtain relevant articles/papers! "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Task_1,2_b.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
